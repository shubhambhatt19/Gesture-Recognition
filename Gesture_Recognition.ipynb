{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8E6ZzT7-9CQ9"
   },
   "source": [
    "# Gesture Recognition\n",
    "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XWg5AhZL-cqu",
    "outputId": "b3946344-e6c2-4728-dedb-78fb0326c6bf"
   },
   "outputs": [],
   "source": [
    "# !pip install opencv-contrib-python\n",
    "# !pip3 install scipy==1.2\n",
    "# !pip3 install livelossplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7vsStYOZ9CRF"
   },
   "outputs": [],
   "source": [
    "### Importing required packages\n",
    "import numpy as np\n",
    "import os\n",
    "# from scipy.misc import imread, imresize\n",
    "import cv2\n",
    "import datetime\n",
    "import warnings\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Conv2D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "# !pip3 install livelossplot \n",
    "from livelossplot import PlotLossesKeras\n",
    "import random as rn\n",
    "import tensorflow as tf\n",
    "tf.enable_v2_behavior()\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p7pEx8RC9CRG"
   },
   "outputs": [],
   "source": [
    "# freezing the values\n",
    "np.random.seed(30)\n",
    "rn.seed(30)\n",
    "# tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5qcT34C9CRH"
   },
   "outputs": [],
   "source": [
    "# importing the csv file comtaining the meta data for the images\n",
    "train_doc = np.random.permutation(open('Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('Project_data/val.csv').readlines())\n",
    "# Setting image path\n",
    "train_path = 'Project_data/train'\n",
    "val_path = 'Project_data/val'\n",
    "\n",
    "curr_dt_time = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ILC7cDlb9CRH",
    "outputId": "23ddd481-42d7-4e4a-cfd2-58426c1e5d77"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = num_train_sequences//batch_size# choose the number of epochs\n",
    "print ('# epochs =', num_epochs)\n",
    "\n",
    "# The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide\n",
    "# the number of next() calls it need to make.\n",
    "\n",
    "steps_per_epoch = int(num_train_sequences/batch_size) if (num_train_sequences%batch_size) == 0 else (num_train_sequences//batch_size) + 1\n",
    "validation_steps = int(num_val_sequences/batch_size) if (num_val_sequences%batch_size) == 0 else (num_val_sequences//batch_size) + 1\n",
    "    \n",
    "print(\"# steps_per_epoch:\", steps_per_epoch)\n",
    "print(\"# validation_steps:\", validation_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "45nQ-Wy69CRJ",
    "outputId": "0fccd0c2-693c-4ebc-ac49-ff725a2ae767"
   },
   "outputs": [],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "\n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "\n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', save_freq='epoch', period=4)\n",
    "\n",
    "LR = ReduceLROnPlateau()\n",
    "\n",
    "early = EarlyStopping(patience=4)\n",
    "\n",
    "callbacks_list = [LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "idphl4am9CRJ"
   },
   "outputs": [],
   "source": [
    "def plot(history):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,4))\n",
    "    axes[0].plot(history.history['loss'])   \n",
    "    axes[0].plot(history.history['val_loss'])\n",
    "    axes[0].legend(['loss','val_loss'])\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    axes[1].plot(history.history['categorical_accuracy'])   \n",
    "    axes[1].plot(history.history['val_categorical_accuracy'])\n",
    "    axes[1].legend(['categorical_accuracy','val_categorical_accuracy'])\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "id": "ZP2p9ksGQrIo",
    "outputId": "e1775134-f926-497e-8582-1825a4f8c56a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"Project_data/train.csv\", header=None)\n",
    "data.columns = [\"temp\"]\n",
    "data['temp'].str.split(\";\", expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z3x46oJm9CRK"
   },
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size, x=20, y=180, z=180):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [i for i in range(0,30,2)] #create a list of image numbers you want to use for a particular video\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = cv2.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "#                     #crop the images and resize them. Note that the images are of 2 different shape \n",
    "#                     #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    if image.shape[1] == 160:\n",
    "                        image = cv2.resize(image[:,20:140,:],(84,84)).astype(np.float32)\n",
    "                    else:\n",
    "                        image = cv2.resize(image,(84,84)).astype(np.float32)\n",
    "\n",
    "#                     #Converting to gray scale\n",
    "#                     temp = temp.mean(axis=-1,keepdims=1) \n",
    "#                     temp = temp/127.5-1 #Normalize data\n",
    "#                     batch_data[folder,idx] = temp #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0] - 104\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1] - 117\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2] - 123                    \n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                \n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if (len(folder_list) != batch_size*num_batches):\n",
    "            print(\"Batch: \",num_batches+1,\"Index:\", batch_size)\n",
    "            batch_size = len(folder_list) - (batch_size*num_batches)\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = cv2.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    if image.shape[1] == 160:\n",
    "                        image = cv2.resize(image[:,20:140,:],(84,84)).astype(np.float32)\n",
    "                    else:\n",
    "                        image = cv2.resize(image,(84,84)).astype(np.float32)\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0] - 104\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1] - 117\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2] - 123                    \n",
    "\n",
    "                   \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BVz9ZmBW6t4Q"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-aRn-En49CRP"
   },
   "outputs": [],
   "source": [
    "input_shape= (18,84,84,3)\n",
    "\n",
    "# mod = VGG16(weights = \"imagenet\", include_top=False)\n",
    "\n",
    "from tensorflow.python.keras.layers import Input, Dense, TimeDistributed, MaxPooling3D, Conv3D\n",
    "\n",
    "\n",
    "# from tf.layers import TimeDistributed, MaxPooling2D, Conv2D\n",
    "\n",
    "def model_1(input_shape = input_shape):\n",
    "    # Define model1\n",
    "    model1 = Sequential()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
    "    # model1.add(TimeDistributed(mod,input_shape=(224, 224, 3)))\n",
    "    model1.add(Conv3D(32, kernel_size=(3,3,3), input_shape=input_shape,padding='same', activation = 'elu', strides=(1,1,1)))\n",
    "    model1.add(BatchNormalization())\n",
    "    model1.add(MaxPooling3D(pool_size=(2,2,1), strides=(2,2,1)))\n",
    "\n",
    "    model1.add(Conv3D(64, kernel_size=(3,3,3),padding='same', activation = 'elu', strides=(1,1,1)))\n",
    "    model1.add(BatchNormalization())\n",
    "    model1.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "    model1.add(Conv3D(128, kernel_size=(3,3,3),padding='same', activation = 'elu', strides=(1,1,1)))\n",
    "    model1.add(BatchNormalization())\n",
    "    model1.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "     \n",
    "    #Flatten Layers\n",
    "    model1.add(Flatten())\n",
    "    model1.add(Dropout(0.5))\n",
    "    model1.add(Dense(256, activation='elu'))\n",
    "    model1.add(Dropout(0.5))\n",
    "    \n",
    "    #softmax layer\n",
    "    model1.add(Dense(5, activation='softmax'))\n",
    "    return model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Xso3Wiv9CRP"
   },
   "outputs": [],
   "source": [
    "optimiser = optimizers.SGD(0.001, decay=1e-6, momentum=0.7, nesterov=True) #write your optimizer\n",
    "model_test1 = model_1(input_shape= input_shape)\n",
    "model_test1.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-zvD5sSX9CRQ"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_generator = generator(train_path, train_doc, batch_size, x=18, y=84, z=84)\n",
    "val_generator = generator(val_path, val_doc, batch_size, x=18, y=84, z=84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "EeHSHHkSY5_0",
    "outputId": "7a49d7e1-a4e2-490f-8eee-37cba5db2376",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model_test1.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=30, verbose=1, validation_data=val_generator, validation_steps=validation_steps, class_weight=None, workers=-1, initial_epoch=0, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = history.history['loss']\n",
    "loss_val = history.history['val_loss']\n",
    "epochs = range(1,31)\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
    "plt.plot(epochs, loss_val, 'b', label='validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "acc_train = history.history['categorical_accuracy']\n",
    "acc_val = history.history['val_categorical_accuracy']\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs, acc_train, 'g', label='Training acc')\n",
    "plt.plot(epochs, acc_val, 'b', label='validation acc')\n",
    "plt.title('Training and Validation acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6IZP23cI9CRQ"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model_test1.evaluate_generator(val_generator, steps=validation_steps, verbose=1)\n",
    "print()\n",
    "print(\"CLASSIFICATION ACCCURACY\")\n",
    "print()\n",
    "print(\"# loss:\",loss)\n",
    "print(\"# accuracy:\",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kiEhT9fWNow"
   },
   "source": [
    "# Using Sequence model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LMer9WUQWNsp"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "from tensorflow.keras.applications import mobilenet\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.layers import TimeDistributed, LSTM, MaxPooling2D\n",
    "\n",
    "m_net = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "for i in m_net.layers:\n",
    "    i.trainable = False\n",
    "    \n",
    "\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(m_net, input_shape = (18, 84, 84, 3)))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(GRU(128), return_sequence = True)\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "\n",
    "optimiser = optimizers.SGD(0.001)\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LOfPaYaxX9iC"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mkur6jo-XiPA"
   },
   "outputs": [],
   "source": [
    "# callbacks_list = [checkpoint, LR]\n",
    "model_seq = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=30, verbose=1, callbacks=callbacks_list, validation_data=val_generator, validation_steps=validation_steps, class_weight=None, workers=-1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = model_seq.history['loss']\n",
    "loss_val = model_seq.history['val_loss']\n",
    "epochs = range(1,31)\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
    "plt.plot(epochs, loss_val, 'b', label='validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "acc_train = model_seq.history['categorical_accuracy']\n",
    "acc_val = model_seq.history['val_categorical_accuracy']\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs, acc_train, 'g', label='Training acc')\n",
    "plt.plot(epochs, acc_val, 'b', label='validation acc')\n",
    "plt.title('Training and Validation acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NHlC2FYIYkVb"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Neural_Nets_Project_Gesture_Recognition_v2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
